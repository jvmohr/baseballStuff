{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests, os, datetime\n",
    "import time as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different files\n",
    "\n",
    "# games.csv - general game info - DONE\n",
    "# hittersByGame.csv - how each player did in each game - DONE\n",
    "# pitchersByGame.csv - how each player did in each game - DONE\n",
    "\n",
    "# plays.csv - batter events - batter singled, batter struck out, etc. - DONE\n",
    "# events.csv - general events - Have event id (per game) to join with next - DONE\n",
    "# pitches.csv - one row per pitch per game - DONE\n",
    "# inningScore.csv - score per inning - DONE\n",
    "# inningHighlights.csv - # of runs, hits, and errors per inning - DONE\n",
    "\n",
    "# hittingNotes.csv - DONE\n",
    "# pitchingNotes.csv - DONE\n",
    "# baserunningNotes.csv - DONE\n",
    "# fieldingNotes.csv - DONE\n",
    "# letterNotes.csv - for notes attached to batters (and maybe pitchers) - DONE\n",
    "\n",
    "# can always add TEAM row to csvs later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveDF(df, file):\n",
    "    global year, iii, all_df_dict\n",
    "    \n",
    "    # update all_df_dict\n",
    "    if file in all_df_dict.keys():\n",
    "        old_df = all_df_dict[file]\n",
    "        new_df = pd.concat([old_df, df], ignore_index=True)\n",
    "    else:\n",
    "        new_df = df\n",
    "    all_df_dict[file] = new_df\n",
    "    \n",
    "    # want to save in chunks of 100 games, so return otherwise\n",
    "    if iii % 100 != 0:\n",
    "        return\n",
    "    \n",
    "    print('writing with iii:', iii)\n",
    "    \n",
    "    path = os.path.join('data', year)\n",
    "    total_path = os.path.join(path, file)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    if os.path.exists(total_path): # if it already exists, get the old one & combine\n",
    "        old_df = pd.read_csv(total_path)\n",
    "        to_write_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        to_write_df = new_df\n",
    "    \n",
    "    to_write_df.to_csv(total_path, index=False)\n",
    "    del all_df_dict[file] # delete dataframe from dict since we just wrote it to a file\n",
    "    return\n",
    "\n",
    "\n",
    "def saveDF2(df, file):\n",
    "    global year, iii\n",
    "    path = os.path.join('data', year)\n",
    "    total_path = os.path.join(path, file)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        new_df = df # ???\n",
    "    \n",
    "    if os.path.exists(total_path): # if it already exists, get the old one & combine\n",
    "        old_df = pd.read_csv(total_path)\n",
    "        new_df = pd.concat([old_df, df], ignore_index=True)\n",
    "    else:\n",
    "        new_df = df\n",
    "    \n",
    "    new_df.to_csv(total_path, index=False)\n",
    "    return\n",
    "\n",
    "def getName(x):\n",
    "    if x.text == 'TEAM':\n",
    "        return x.text\n",
    "    \n",
    "    if len(x.contents) == 3:\n",
    "        return x.contents[1].text\n",
    "    else:\n",
    "        return x.contents[0].text\n",
    "\n",
    "def getPos(x):\n",
    "    if x.text == 'TEAM':\n",
    "        return x.text\n",
    "    \n",
    "    if len(x.contents) == 3:\n",
    "        return x.contents[2]\n",
    "    else:\n",
    "        return x.contents[1]\n",
    "    \n",
    "def loadPage(url):\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            r.raise_for_status()\n",
    "            break\n",
    "        except:\n",
    "            print('AT:', url)\n",
    "            tm.sleep(2)\n",
    "    return BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "# plays\n",
    "def scrapePitchByPitchPage(gameid):\n",
    "    url = 'https://www.espn.com/mlb/playbyplay/_/gameId/{}'.format(gameid)\n",
    "    soup = loadPage(url)\n",
    "\n",
    "    s = soup.find_all('div', attrs={'data-module':'playbyplay'})[0]\n",
    "    \n",
    "    all_pitches = []\n",
    "    events = []\n",
    "    inning_highlights = []\n",
    "    header = []\n",
    "\n",
    "    event_id = 0\n",
    "    # each half-inning\n",
    "    for inning in s.find_all('section'):\n",
    "        if 'allPlays' not in inning.get('id'):\n",
    "            continue\n",
    "\n",
    "        current_pitcher = ''\n",
    "        current_pitching_team = ''\n",
    "        current_hitting_team = ''\n",
    "        info = inning.find('h1').text.split(' - ')\n",
    "        # each hitter in half-inning\n",
    "        for batter in inning.find_all('li'):\n",
    "            if 'accordion-item' in batter.get('class'): # get pitches\n",
    "                t = batter.find_all('table')[0]\n",
    "\n",
    "                # get the header - can rip this out of the loop\n",
    "                header = ['Num']\n",
    "                for row in t.find('thead').find_all('th'):\n",
    "                    header.append(row.text if row.text != '' else row.get('class')[0])\n",
    "\n",
    "                # get all the pitches to this batter\n",
    "                pitches = []\n",
    "                for row in t.find('tbody').find_all('tr'):\n",
    "                    pitch = []\n",
    "                    tds = row.find_all('td')\n",
    "\n",
    "                    pitch.append(tds[0].find('span').text) # pitch number\n",
    "                    pitch.append(tds[0].contents[1]) # call\n",
    "                    pitch.append(tds[1].text) # type\n",
    "                    pitch.append(row.find('td', attrs={'class':'mph'}).text) # mph\n",
    "\n",
    "                    # pitch location\n",
    "                    try:\n",
    "                        pitch.append(row.find('span', attrs={'class':'pitch-location'}).get('style')) # pitch location (top - 0 seems to be the top)\n",
    "                    except:\n",
    "                        pitch.append('NA')\n",
    "\n",
    "                    # bases\n",
    "                    bases = ''\n",
    "                    for x in row.find('td', attrs={'class':'play-bases'}).find_all('span'):\n",
    "                        if 'active' in str(x):\n",
    "                            bases += x.get('class')[1].split('--')[1]\n",
    "                    pitch.append(bases) # bases\n",
    "\n",
    "                    # hit location\n",
    "                    try:\n",
    "                        pitch.append(row.find('span', attrs={'class':'hit-location'}).get('style')) # hit location\n",
    "                    except:\n",
    "                        pitch.append('')\n",
    "\n",
    "                    pitch.append(current_pitcher)\n",
    "                    pitch.append(current_pitching_team)\n",
    "                    pitch.append(current_hitting_team) # team\n",
    "                    pitch.append(info[1]) # inning\n",
    "                    pitch.append(event_id)\n",
    "                    pitches.append(pitch)\n",
    "                all_pitches.extend(pitches)\n",
    "            # get inning summary - # of runs, hits, and errors\n",
    "            elif 'info-row--footer' in batter.get('class'):\n",
    "                cats = batter.text.split(', ')\n",
    "                inning_highlights.append([batter.get('id').replace('allPlays', '').replace('Linescore', '').replace('op', '').replace('ottom', ''), \n",
    "                                          int(cats[0].split(' ')[0]), int(cats[1].split(' ')[0]), int(cats[2].split(' ')[0])])\n",
    "                continue\n",
    "\n",
    "            hl = batter.find('span', attrs={'class':'headline'}).text\n",
    "            home = batter.find('span', attrs={'class':'home'}).text\n",
    "            away = batter.find('span', attrs={'class':'away'}).text\n",
    "            \n",
    "            # beginning of inning or change of pitchers\n",
    "            if 'pitching for' in hl:\n",
    "                current_pitcher = hl.split(' pitching for ')[0]\n",
    "                current_pitching_team = hl.split(' pitching for ')[1]\n",
    "                current_hitting_team = home if current_pitching_team == away else away \n",
    "            \n",
    "            events.append([gameid, current_pitching_team, current_hitting_team, info[1], event_id, hl, home, away])\n",
    "            event_id += 1\n",
    "\n",
    "    # no events listed - didn't make it into loop\n",
    "    if header == []:\n",
    "        return False\n",
    "    \n",
    "    # inningHighlights.csv\n",
    "    inning_highlights_df = pd.DataFrame(inning_highlights, columns=['Inning', 'Runs', 'Hits', 'Errors'])\n",
    "    inning_highlights_df['Game'] = gameid\n",
    "\n",
    "    saveDF(inning_highlights_df, 'inningHighlights.csv') # inningHighlights.csv -------------------------------\n",
    "\n",
    "    # events.csv\n",
    "    events_df = pd.DataFrame(events, columns=['Game', 'Pitching Team', 'Batting Team', \n",
    "                                              'Inning', 'Event Id', 'Events', 'Away', 'Home']) # events.csv\n",
    "    saveDF(events_df, 'events.csv') # events.csv ------------------------------------------------------\n",
    "    \n",
    "    # pitches.csv\n",
    "    header.extend(['Pitcher', 'Pitching Team', 'Batting Team', 'Inning', 'Event Id'])\n",
    "    pitches_df = pd.DataFrame(all_pitches, columns=header)\n",
    "    pitches_df['Game'] = gameid\n",
    "    saveDF(pitches_df, 'pitches.csv') # pitches.csv ---------------------------------------------------\n",
    "    return True\n",
    "\n",
    "# stats\n",
    "def scrapeBoxScorePage(gameid, game_stats):\n",
    "    url = 'https://www.espn.com/mlb/boxscore/_/gameId/{}'.format(gameid) # https://www.espn.com/mlb/boxscore/_/gameId/401227647\n",
    "    soup1 = loadPage(url)\n",
    "\n",
    "    # Inning Score - inningScore.csv\n",
    "    inning_score = pd.read_html(str(soup1.find('table', attrs={'class':'linescore__table'})))[0] # new csv (put H, E in games.csv?)\n",
    "    inning_score = inning_score.rename(columns={'Unnamed: 0': 'Team'})\n",
    "    inning_score['Game'] = gameid\n",
    "    inning_score['Team'] = inning_score['Team'].apply(lambda x: x.split(' ')[-1])\n",
    "\n",
    "    saveDF(inning_score, 'inningScore.csv') # inningScore.csv --------------------------------------\n",
    "\n",
    "    # win/loss/save pitchers\n",
    "    pitchers = soup1.find('div', attrs={'class':'linescore__situation-container'}).find_all('div', attrs={'class':'linescore__player_stats'})\n",
    "\n",
    "    for pitcher in pitchers:\n",
    "        cond = pitcher.contents[0].text # win/lose/save\n",
    "        game_stats[cond+' - Pitcher - Stats'] = pitcher.contents[2].text # ip/k/etc\n",
    "        game_stats[cond+' - Pitcher - Id'] = pitcher.find('a').get('href').split('/')[-1]\n",
    "        game_stats[cond+' - Pitcher - Name'] = pitcher.contents[1].find('span', attrs={'class':'fullName'}).text\n",
    "        game_stats[cond+' - Pitcher - AbbrName'] = pitcher.contents[1].find('span', attrs={'class':'abbrName'}).text\n",
    "        try:\n",
    "            game_stats[cond+' - Pitcher - Record'] = pitcher.contents[1].find('span', attrs={'class':'stat'}).text\n",
    "        except:\n",
    "            game_stats[cond+' - Pitcher - Record'] = 'NA'\n",
    "\n",
    "\n",
    "    # hitters/pitchers\n",
    "    player_stats = soup1.find('div', attrs={'data-module':'boxscore'})\n",
    "    stat_groups = player_stats.find_all('article', attrs={'class':'sub-module boxscore-2017'})\n",
    "\n",
    "    # letterNotes.csv\n",
    "    notes = soup1.find_all('div', attrs={'data-note-id':True}) # https://stackoverflow.com/a/45365599\n",
    "    final_notes = []\n",
    "\n",
    "    for x in notes:\n",
    "        note_id = x.get('data-note-id')\n",
    "        final_notes.append([gameid, note_id.split('-')[0], note_id.split('-')[1], x.text])\n",
    "\n",
    "    letter_notes_df = pd.DataFrame(final_notes, columns=['Game', 'Player Id', 'Player-Note Id', 'Note'])\n",
    "    saveDF(letter_notes_df, 'letterNotes.csv') # letterNotes.csv --------------------------------------\n",
    "\n",
    "\n",
    "    # batting\n",
    "    team = 'away'\n",
    "    batting_notes = []\n",
    "    for hitting in stat_groups[0], stat_groups[2]:\n",
    "        # pitcher stats\n",
    "        hitting_df = pd.read_html(str(hitting))[0]\n",
    "        hitting_df['Game'] = gameid\n",
    "        hitting_df['Team'] = game_stats[team]\n",
    "\n",
    "        # to get names and positions in separate columns\n",
    "        snames = hitting.find_all('td', attrs={'class':'name'})\n",
    "        names = [getName(x) for x in snames]\n",
    "        pos = [getPos(x) for x in snames]\n",
    "\n",
    "        hitting_df['Hitters'] = names\n",
    "        hitting_df['Position'] = pos\n",
    "        hitting_df['Hitter Id'] = [x.get('data-athlete-id') for x in hitting.find_all('tbody', class_='athletes')] + ['-']\n",
    "\n",
    "        # extra batting info\n",
    "        p_stats = hitting.find('div', attrs={'data-type':'battingDetails'}).find_all('li')[1:]\n",
    "        for stat in p_stats:\n",
    "            batting_notes.append([gameid, game_stats[team], stat.contents[0].text.replace(':', ''), stat.contents[1]])\n",
    "\n",
    "        team = 'home'\n",
    "\n",
    "    batting_notes_df = pd.DataFrame(batting_notes, columns=['Game', 'Team', 'Stat', 'Data'])\n",
    "    saveDF(batting_notes_df, 'hittingNotes.csv') # hittingNotes.csv ---------------------------------\n",
    "    saveDF(hitting_df, 'hittersByGame.csv') # hittersByGame.csv -------------------------------------\n",
    "\n",
    "    # pitching\n",
    "    team = 'away'\n",
    "    pitching_notes = []\n",
    "    for pitching in stat_groups[1], stat_groups[3]:\n",
    "        # pitcher stats\n",
    "        pitching_df = pd.read_html(str(pitching))[0] # rip team row into games.csv?\n",
    "        pitching_df['Game'] = gameid\n",
    "        pitching_df['Team'] = game_stats[team]\n",
    "\n",
    "        # split record off\n",
    "        pitching_df['Extra'] = pitching_df['Pitchers'].apply(lambda x: '' if '(' not in x else '(' + x.split(' (')[1])\n",
    "        pitching_df['Pitchers'] = pitching_df['Pitchers'].apply(lambda x: x if '(' not in x else x.split(' (')[0])\n",
    "        pitching_df['Pitcher Id'] = [x.get('data-athlete-id') for x in pitching.find_all('tbody', class_='athletes')] + ['-']\n",
    "\n",
    "        # extra pitching info\n",
    "        p_stats = pitching.find('div', attrs={'data-type':'pitchingDetails'}).find_all('li')[1:]\n",
    "        for stat in p_stats:\n",
    "            pitching_notes.append([gameid, game_stats[team], stat.contents[0].text.replace(':', ''), stat.contents[1]])\n",
    "\n",
    "        team = 'home'\n",
    "\n",
    "    pitching_notes_df = pd.DataFrame(pitching_notes, columns=['Game', 'Team', 'Stat', 'Data'])\n",
    "    saveDF(pitching_notes_df, 'pitchingNotes.csv') # pitchingNotes.csv -------------------------------\n",
    "    saveDF(pitching_df, 'pitchersByGame.csv') # pitchersByGame.csv -----------------------------------\n",
    "\n",
    "\n",
    "    # could put the next part in a fcn\n",
    "    # baserunning\n",
    "    baserunning_notes = []\n",
    "    team = 'away'\n",
    "    for ba in soup1.find_all('div', attrs={'data-type':'baserunningDetails'}):\n",
    "        ba_stats = ba.find_all('li')[1:]\n",
    "\n",
    "        for stat in ba_stats:\n",
    "            baserunning_notes.append([gameid, game_stats[team], stat.contents[0].text.replace(':', ''), stat.contents[1]])\n",
    "\n",
    "        team = 'home'\n",
    "\n",
    "    baserunning_notes_df = pd.DataFrame(baserunning_notes, columns=['Game', 'Team', 'Stat', 'Data'])\n",
    "    saveDF(baserunning_notes_df, 'baserunningNotes.csv') # baserunningNotes.csv -----------------------\n",
    "\n",
    "\n",
    "    # fielding\n",
    "    fielding_notes = []\n",
    "    team = 'away'\n",
    "    for fi in soup1.find_all('div', attrs={'data-type':'fieldingDetails'}):\n",
    "        fi_stats = fi.find_all('li')[1:]\n",
    "\n",
    "        for stat in fi_stats:\n",
    "            fielding_notes.append([gameid, game_stats[team], stat.contents[0].text.replace(':', ''), stat.contents[1]])\n",
    "\n",
    "        team = 'home'\n",
    "\n",
    "    fielding_notes_df = pd.DataFrame(fielding_notes, columns=['Game', 'Team', 'Stat', 'Data'])\n",
    "    saveDF(fielding_notes_df, 'fieldingNotes.csv') # fieldingNotes.csv -----------------------------------\n",
    "    return game_stats\n",
    "\n",
    "# main page\n",
    "def scrapeMainPage(gameid, canceled=False):\n",
    "    url = 'https://www.espn.com/mlb/game/_/gameId/{}'.format(gameid)\n",
    "    soup2 = loadPage(url)\n",
    "    \n",
    "    try:\n",
    "        if soup2.find('div', attrs={'class':'game-details header'}).text.upper() == 'SPRING TRAINING':\n",
    "            print('Spring Training:', gameid)\n",
    "            return {}, True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    game_stats = {'Game': gameid}\n",
    "\n",
    "    # Team Info & Final Score\n",
    "    top = soup2.find('div', attrs={'id':'gamepackage-matchup-wrap'})\n",
    "\n",
    "    place = 'away'\n",
    "    for team in top.find_all('div', attrs={'class':'team-container'}):\n",
    "        record = team.find('div', attrs={'class':'record'}).text.split(', ')\n",
    "        try:\n",
    "            name = team.find('a', attrs={'class':'team-name'}).contents # also has city and team name\n",
    "        except:\n",
    "            name = team.find('div', attrs={'class':'team-name'}).contents # for All-Star games\n",
    "\n",
    "        game_stats[place] = name[2].text\n",
    "        game_stats[place+'-record'] = record[0]\n",
    "        if len(record) > 1:\n",
    "            game_stats[place+place+'-record'] = record[1]\n",
    "\n",
    "        place = 'home' # switch\n",
    "\n",
    "    scores = top.find_all('div', attrs={'class':'score-container'})\n",
    "    game_stats['away-score'] = scores[0].text\n",
    "    game_stats['home-score'] = scores[1].text\n",
    "\n",
    "    if soup2.find('div', attrs={'class':'game-details header'}) is not None:\n",
    "        game_stats['postseason info'] = soup2.find('div', attrs={'class':'game-details header'}).text\n",
    "    else:\n",
    "        game_stats['postseason info'] = ''\n",
    "    \n",
    "    # Statistics (from bar graphs)\n",
    "    stats = soup2.find('div', attrs={'class': 'sub-module team-statistics'})\n",
    "    if not canceled: # is none for canceled games\n",
    "        for bars in stats.find_all('li', attrs={'class': 'stat-box'}):\n",
    "            stat = bars.find('h3').text\n",
    "\n",
    "            values = bars.find_all('span', attrs={'class':'chartValue'})\n",
    "            game_stats[stat + ' - Away'] = values[0].text # away\n",
    "            game_stats[stat + ' - Home'] = values[1].text # home\n",
    "    \n",
    "    # Game Info *********************************\n",
    "    gs = soup2.find('article', attrs={'class':'sub-module game-information'})\n",
    "    try:\n",
    "        game_stats['Stadium'] = gs.find('div', attrs={'class':'game-location'}).text # stadium\n",
    "    except:\n",
    "        game_stats['Stadium'] = gs.find('div', attrs={'class':'game-field'}).text # stadium\n",
    "    \n",
    "    game_stats['Date'] = gs.find('span', attrs={'data-behavior':'date_time'}).get('data-date') # date\n",
    "    \n",
    "    try:\n",
    "        game_stats['Location'] = gs.find('li', attrs={'class':'icon-font-before icon-location-solid-before'}).text.strip() # city, state\n",
    "    except:\n",
    "        game_stats['Location'] = 'NA'\n",
    "    \n",
    "    # odds\n",
    "    try:\n",
    "        odds = gs.find('div', attrs={'class':'odds-lines-plus-logo'}).text.strip().split('\\n') # odds\n",
    "        game_stats['Odds'] = odds[0]\n",
    "        game_stats['O/U'] = odds[1]\n",
    "    except:\n",
    "        game_stats['Odds'] = 'NA'\n",
    "        game_stats['O/U'] = 'NA'\n",
    "\n",
    "    # Attendance/Capacity\n",
    "    attcap = gs.find_all('div', attrs={'class':'game-info-note capacity'})\n",
    "    for ac in attcap:\n",
    "        if 'Capacity' in ac.text:\n",
    "            game_stats['Capacity'] = ac.text.split(': ')[1] # capacity\n",
    "        elif 'Attendance' in ac.text:\n",
    "            game_stats['Attendance'] = ac.text.split(': ')[1] # attendance\n",
    "    \n",
    "    # Duration and Umps\n",
    "    extra_info = gs.find_all('div', attrs={'class':'game-info-note__container'})\n",
    "    for ei in extra_info:\n",
    "        if 'Game Time' in ei.text:\n",
    "            game_stats['Duration'] = ei.text.split(': ')[1] # game time\n",
    "        elif 'Umpires' in ei.text:\n",
    "            game_stats['Umpires'] = ei.text.split(': ')[1] # umpires\n",
    "\n",
    "    if canceled:\n",
    "        game_stats['Duration'] = 'Canceled'\n",
    "    \n",
    "    # How each batter did in each at bat (or mention if they had no at-bats)\n",
    "    if not canceled:\n",
    "        rolling = pd.DataFrame()\n",
    "        team = game_stats['away']\n",
    "        for j in [0, 3]: # 0 is away, 3 is home\n",
    "            # find all of the rows\n",
    "            for i, row in enumerate(soup2.find_all('table')[j].find_all('tr')[1:]):\n",
    "                # alternates between name and at-bats\n",
    "                if i % 2 == 0:\n",
    "                    name = row.find('span', attrs={'class':'name'}).text\n",
    "                    player_id = row.find('div', attrs={'class':'accordion-item'}).get('data-key').split('-')[1]\n",
    "                else:\n",
    "                    bats = row.find_all('li')\n",
    "                    bats = [(player_id, name, x.text) for x in bats]\n",
    "\n",
    "                    temp = pd.DataFrame(bats)\n",
    "                    temp['Team'] = team\n",
    "                    rolling = pd.concat([rolling, temp])\n",
    "\n",
    "            team = game_stats['home'] # switch to home team for second iteration\n",
    "\n",
    "        rolling.columns = ['Batter Id', 'Batter', 'Event', 'Team']\n",
    "        rolling['Game'] = gameid\n",
    "        rolling = rolling[['Game', 'Team', 'Batter Id', 'Batter', 'Event']]\n",
    "        rolling = rolling.reset_index(drop=True)\n",
    "        saveDF(rolling, 'plays.csv') # plays.csv -------------------------------------------\n",
    "    return game_stats, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Thursday, April 1 **********\n",
      "Postponed 401227054, skipped\n",
      "Postponed 401227060, skipped\n",
      "********** Friday, April 2 **********\n",
      "********** Saturday, April 3 **********\n",
      "Postponed 401227081, skipped\n",
      "********** Sunday, April 4 **********\n",
      "Postponed 401227096, skipped\n",
      "********** Monday, April 5 **********\n",
      "Postponed 401227101, skipped\n",
      "********** Tuesday, April 6 **********\n",
      "********** Wednesday, April 7 **********\n",
      "********** Thursday, April 8 **********\n",
      "********** Friday, April 9 **********\n",
      "********** Saturday, April 10 **********\n",
      "Postponed 401227167, skipped\n",
      "********** Sunday, April 11 **********\n",
      "Postponed 401227176, skipped\n",
      "********** Monday, April 12 **********\n",
      "Postponed 401227191, skipped\n",
      "Postponed 401227201, skipped\n",
      "Postponed 401227199, skipped\n",
      "********** Tuesday, April 13 **********\n",
      "********** Wednesday, April 14 **********\n",
      "Postponed 401227230, skipped\n",
      "********** Thursday, April 15 **********\n",
      "Postponed 401227239, skipped\n",
      "********** Friday, April 16 **********\n",
      "Postponed 401227247, skipped\n",
      "Postponed 401227258, skipped\n",
      "Postponed 401227253, skipped\n",
      "********** Saturday, April 17 **********\n",
      "Postponed 401227267, skipped\n",
      "********** Sunday, April 18 **********\n",
      "Postponed 401227282, skipped\n",
      "********** Monday, April 19 **********\n",
      "Postponed 401227292, skipped\n",
      "********** Tuesday, April 20 **********\n",
      "Postponed 401227306, skipped\n",
      "********** Wednesday, April 21 **********\n",
      "Postponed 401227315, skipped\n",
      "********** Thursday, April 22 **********\n",
      "********** Friday, April 23 **********\n",
      "********** Saturday, April 24 **********\n",
      "Postponed 401227350, skipped\n",
      "********** Sunday, April 25 **********\n",
      "********** Monday, April 26 **********\n",
      "********** Tuesday, April 27 **********\n",
      "********** Wednesday, April 28 **********\n",
      "Postponed 401227410, skipped\n",
      "********** Thursday, April 29 **********\n",
      "********** Friday, April 30 **********\n",
      "********** Saturday, May 1 **********\n",
      "********** Sunday, May 2 **********\n",
      "********** Monday, May 3 **********\n",
      "Postponed 401227476, skipped\n",
      "Postponed 401227480, skipped\n",
      "********** Tuesday, May 4 **********\n",
      "Postponed 401227493, skipped\n",
      "********** Wednesday, May 5 **********\n",
      "********** Thursday, May 6 **********\n",
      "********** Friday, May 7 **********\n",
      "********** Saturday, May 8 **********\n",
      "********** Sunday, May 9 **********\n",
      "Postponed 401227557, skipped\n",
      "Postponed 401227561, skipped\n",
      "********** Monday, May 10 **********\n",
      "Postponed 401227573, skipped\n",
      "********** Tuesday, May 11 **********\n",
      "********** Wednesday, May 12 **********\n",
      "********** Thursday, May 13 **********\n",
      "********** Friday, May 14 **********\n",
      "********** Saturday, May 15 **********\n",
      "********** Sunday, May 16 **********\n",
      "********** Monday, May 17 **********\n",
      "********** Tuesday, May 18 **********\n",
      "********** Wednesday, May 19 **********\n",
      "********** Thursday, May 20 **********\n",
      "********** Friday, May 21 **********\n",
      "********** Saturday, May 22 **********\n",
      "********** Sunday, May 23 **********\n",
      "********** Monday, May 24 **********\n",
      "********** Tuesday, May 25 **********\n",
      "********** Wednesday, May 26 **********\n",
      "Postponed 401227791, skipped\n",
      "Postponed 401227782, skipped\n",
      "********** Thursday, May 27 **********\n",
      "********** Friday, May 28 **********\n",
      "Postponed 401227809, skipped\n",
      "Postponed 401227812, skipped\n",
      "Postponed 401227806, skipped\n",
      "Postponed 401227807, skipped\n",
      "********** Saturday, May 29 **********\n",
      "Postponed 401227833, skipped\n",
      "********** Sunday, May 30 **********\n",
      "Postponed 401227840, skipped\n",
      "Postponed 401227835, skipped\n",
      "********** Monday, May 31 **********\n",
      "********** Tuesday, June 1 **********\n",
      "********** Wednesday, June 2 **********\n",
      "Postponed 401227883, skipped\n",
      "Postponed 401227878, skipped\n",
      "********** Thursday, June 3 **********\n",
      "********** Friday, June 4 **********\n",
      "********** Saturday, June 5 **********\n",
      "********** Sunday, June 6 **********\n",
      "********** Monday, June 7 **********\n",
      "********** Tuesday, June 8 **********\n",
      "********** Wednesday, June 9 **********\n",
      "********** Thursday, June 10 **********\n",
      "Postponed 401227986, skipped\n",
      "********** Friday, June 11 **********\n",
      "********** Saturday, June 12 **********\n",
      "********** Sunday, June 13 **********\n",
      "********** Monday, June 14 **********\n",
      "********** Tuesday, June 15 **********\n",
      "********** Wednesday, June 16 **********\n",
      "********** Thursday, June 17 **********\n",
      "********** Friday, June 18 **********\n",
      "********** Saturday, June 19 **********\n",
      "Postponed 401228112, skipped\n",
      "********** Sunday, June 20 **********\n",
      "********** Monday, June 21 **********\n",
      "********** Tuesday, June 22 **********\n",
      "********** Wednesday, June 23 **********\n",
      "********** Thursday, June 24 **********\n",
      "********** Friday, June 25 **********\n",
      "Postponed 401228182, skipped\n",
      "********** Saturday, June 26 **********\n",
      "Postponed 401228195, skipped\n",
      "********** Sunday, June 27 **********\n",
      "********** Monday, June 28 **********\n",
      "Postponed 401228226, skipped\n",
      "********** Tuesday, June 29 **********\n",
      "Postponed 401228233, skipped\n",
      "********** Wednesday, June 30 **********\n",
      "********** Thursday, July 1 **********\n",
      "Postponed 401228258, skipped\n",
      "Postponed 401228262, skipped\n",
      "********** Friday, July 2 **********\n",
      "Postponed 401228280, skipped\n",
      "********** Saturday, July 3 **********\n",
      "********** Sunday, July 4 **********\n",
      "********** Monday, July 5 **********\n",
      "********** Tuesday, July 6 **********\n",
      "Postponed 401228335, skipped\n",
      "Postponed 401228331, skipped\n",
      "********** Wednesday, July 7 **********\n",
      "********** Thursday, July 8 **********\n",
      "Postponed 401228366, skipped\n",
      "Postponed 401228365, skipped\n",
      "********** Friday, July 9 **********\n",
      "********** Saturday, July 10 **********\n",
      "********** Sunday, July 11 **********\n",
      "Postponed 401228405, skipped\n",
      "Postponed 401228410, skipped\n",
      "********** Monday, July 12 **********\n",
      "********** Tuesday, July 13 **********\n",
      "********** Wednesday, July 14 **********\n",
      "********** Thursday, July 15 **********\n",
      "Postponed 401228413, skipped\n",
      "********** Friday, July 16 **********\n",
      "Postponed 401325780, skipped\n",
      "Postponed 401228422, skipped\n",
      "********** Saturday, July 17 **********\n",
      "Postponed 401228443, skipped\n",
      "********** Sunday, July 18 **********\n",
      "********** Monday, July 19 **********\n",
      "Postponed 401228468, skipped\n",
      "********** Tuesday, July 20 **********\n",
      "Postponed 401228473, skipped\n",
      "********** Wednesday, July 21 **********\n",
      "********** Thursday, July 22 **********\n",
      "********** Friday, July 23 **********\n",
      "********** Saturday, July 24 **********\n",
      "********** Sunday, July 25 **********\n",
      "********** Monday, July 26 **********\n",
      "********** Tuesday, July 27 **********\n",
      "Postponed 401228575, skipped\n",
      "********** Wednesday, July 28 **********\n",
      "Postponed 401228591, skipped\n",
      "********** Thursday, July 29 **********\n",
      "********** Friday, July 30 **********\n",
      "********** Saturday, July 31 **********\n",
      "********** Sunday, August 1 **********\n",
      "********** Monday, August 2 **********\n",
      "********** Tuesday, August 3 **********\n",
      "********** Wednesday, August 4 **********\n",
      "********** Thursday, August 5 **********\n",
      "********** Friday, August 6 **********\n",
      "********** Saturday, August 7 **********\n",
      "********** Sunday, August 8 **********\n",
      "********** Monday, August 9 **********\n",
      "Postponed 401228745, skipped\n",
      "********** Tuesday, August 10 **********\n",
      "3.12\n",
      "2.68\n",
      "3.28\n",
      "4.4\n",
      "3.14\n",
      "3.1\n",
      "3.29\n",
      "3.41\n",
      "3.02\n",
      "4.84\n",
      "3.32\n",
      "3.91\n",
      "3.38\n",
      "2.92\n",
      "2.82\n",
      "3.38\n",
      "8.26\n",
      "********** Wednesday, August 11 **********\n",
      "2.94\n",
      "3.01\n",
      "3.0\n",
      "2.8\n",
      "2.89\n",
      "3.04\n",
      "2.75\n",
      "2.98\n",
      "3.03\n",
      "3.18\n",
      "4.64\n",
      "Postponed 401228776, skipped\n",
      "3.01\n",
      "2.77\n",
      "3.21\n",
      "3.57\n",
      "********** Thursday, August 12 **********\n",
      "3.51\n",
      "3.22\n",
      "2.74\n",
      "4.61\n",
      "13.18\n",
      "5.91\n",
      "3.03\n",
      "3.04\n",
      "2.94\n",
      "2.95\n",
      "2.8\n",
      "3.69\n",
      "3.25\n",
      "3.12\n",
      "********** Friday, August 13 **********\n",
      "3.29\n",
      "5.73\n",
      "Postponed 401228797, skipped\n",
      "5.87\n",
      "3.48\n",
      "3.34\n",
      "3.34\n",
      "3.02\n",
      "2.99\n",
      "3.59\n",
      "2.97\n",
      "3.59\n",
      "3.13\n",
      "3.1\n",
      "********** Saturday, August 14 **********\n",
      "2.83\n",
      "2.77\n",
      "2.83\n",
      "4.58\n",
      "4.36\n",
      "2.92\n",
      "3.55\n",
      "3.72\n",
      "2.97\n",
      "3.38\n",
      "2.71\n",
      "2.88\n",
      "4.24\n",
      "3.25\n",
      "3.56\n",
      "3.11\n",
      "********** Sunday, August 15 **********\n",
      "2.96\n",
      "3.03\n",
      "3.37\n",
      "3.02\n",
      "2.85\n",
      "2.75\n",
      "4.89\n",
      "3.28\n",
      "2.89\n",
      "2.84\n",
      "3.02\n",
      "2.92\n",
      "3.15\n",
      "2.75\n",
      "2.79\n",
      "********** Monday, August 16 **********\n",
      "4.93\n",
      "3.22\n",
      "3.26\n",
      "2.87\n",
      "2.71\n",
      "3.15\n",
      "2.63\n",
      "2.68\n",
      "3.37\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "writing with iii: 100\n",
      "6.84\n"
     ]
    }
   ],
   "source": [
    "# ---------------- NEED TO BE CHANGED ----------------------\n",
    "url_date =  \"20210401\" # start date\n",
    "stop_date = \"20210816\" # end date\n",
    "\n",
    "#url_date =  \"20200723\" # start date\n",
    "#stop_date = \"20201028\" # end date\n",
    "\n",
    "#url_date =  \"20190320\" # start date\n",
    "#stop_date = \"20191030\" # end date\n",
    "\n",
    "#url_date =  \"20180329\" # start date\n",
    "#stop_date = \"20181028\" # end date\n",
    "\n",
    "#url_date =  \"20170402\" # start date\n",
    "#stop_date = \"20171101\" # end date\n",
    "\n",
    "#url_date =  \"20160403\" # start date\n",
    "#stop_date = \"20161102\" # end date\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "all_df_dict = {}\n",
    "\n",
    "three_days = datetime.timedelta(3)\n",
    "year = url_date[:4] # used by saveDF\n",
    "url = \"https://www.espn.com/mlb/schedule/_/date/{}\"\n",
    "no_events = []\n",
    "\n",
    "# get list of games data is present for\n",
    "try:\n",
    "    games_df = pd.read_csv(os.path.join('data', year, 'games.csv'))\n",
    "    list_of_games = games_df['Game'].unique()\n",
    "except:\n",
    "    list_of_games = []\n",
    "\n",
    "# Main Loop\n",
    "iii = 0\n",
    "while True: \n",
    "    # stopping condition\n",
    "    if url_date > stop_date:\n",
    "        break\n",
    "    \n",
    "    tm.sleep(1)\n",
    "        \n",
    "    # print('Looking at...', url_date)\n",
    "    soup = loadPage(url.format(url_date)) # Get page for 3 days of games\n",
    "    \n",
    "    # get inner page container\n",
    "    stuff = soup.find_all('div', attrs={'id':'sched-container'})[0]\n",
    "    \n",
    "    current_date = datetime.datetime.strptime(url_date, \"%Y%m%d\")\n",
    "    \n",
    "    # if there are no games, keep going\n",
    "    if stuff.text == 'No games scheduled':\n",
    "        url_date = (current_date + three_days).strftime(\"%Y%m%d\")\n",
    "        continue\n",
    "    \n",
    "    # zip each chunk of games with it's header (the date)\n",
    "    for h2, table in zip(stuff.find_all('h2'), stuff.find_all('table')):\n",
    "        games = table.find_all('tr', class_=['even', 'odd'])\n",
    "        print(10* '*' + ' ' + h2.text + ' ' + 10 * '*')\n",
    "        \n",
    "        # some problems have arisen, so if day is past stop_date, just break\n",
    "        if datetime.datetime.strptime(h2.text+\" {}\".format(url_date[:4]), \"%A, %B %d %Y\").strftime(\"%Y%m%d\") > stop_date:\n",
    "            break\n",
    "        \n",
    "        # go through each game\n",
    "        for game in games:\n",
    "            if game.find('td', attrs={'class':'tickets'}) is not None: # game didn't start\n",
    "                # print('Tickets {} - future game, skipped'.format(gameid))\n",
    "                continue\n",
    "            elif game.find('td', attrs={'class':'live'}) is not None: # game is live\n",
    "                print('Live {}, skipped'.format(gameid))\n",
    "                continue\n",
    "            elif len(game.find_all('td')) == 1: # result row during postseason\n",
    "                continue\n",
    "            \n",
    "            gameid = game.find_all('td')[2].find('a').get('href').split('/')[-1]\n",
    "            # all star game is different\n",
    "            result = game.find_all('a')[4].text if len(game.find_all('a')) != 4 else game.find_all('a')[0].text \n",
    "            canceled = False\n",
    "            \n",
    "            # skip if game was postponed\n",
    "            if result == 'Postponed':\n",
    "                print('Postponed {}, skipped'.format(gameid))\n",
    "                continue\n",
    "            elif result == 'Canceled':\n",
    "                canceled = True\n",
    "            \n",
    "            # if data has already been collected for game, continue\n",
    "            if int(gameid) in list_of_games:\n",
    "                continue\n",
    "            \n",
    "            # print(gameid)\n",
    "            iii += 1\n",
    "            ttt = tm.time()\n",
    "            \n",
    "            # scrape each of three pages\n",
    "            game_stats, spring_training = scrapeMainPage(gameid, canceled)\n",
    "            if spring_training:\n",
    "                continue\n",
    "            if not canceled:\n",
    "                game_stats = scrapeBoxScorePage(gameid, game_stats)\n",
    "                events_flag = scrapePitchByPitchPage(gameid)\n",
    "                if not events_flag:\n",
    "                    no_events.append(gameid)\n",
    "            \n",
    "            # extra innings flag\n",
    "            if 'F/' in result:\n",
    "                game_stats['Extra Innings'] = True\n",
    "            else:\n",
    "                False\n",
    "            \n",
    "            games_df = pd.DataFrame.from_dict(game_stats, orient='index').T \n",
    "            saveDF(games_df, 'games.csv') # games.csv --------------------------------------------\n",
    "            print(round((tm.time() - ttt), 2))\n",
    "    \n",
    "    # move url_date forward\n",
    "    url_date = (current_date + three_days).strftime(\"%Y%m%d\")\n",
    "    \n",
    "# flush all_df_dict\n",
    "path = os.path.join('data', year)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "for key in all_df_dict:\n",
    "    total_path = os.path.join(path, key) \n",
    "    df = all_df_dict[key] # get df\n",
    "    \n",
    "    if os.path.exists(total_path): # if it already exists, get the old one & combine\n",
    "        old_df = pd.read_csv(total_path)\n",
    "        to_write_df = pd.concat([old_df, df], ignore_index=True)\n",
    "    else:\n",
    "        to_write_df = df\n",
    "    \n",
    "    to_write_df.to_csv(total_path, index=False)\n",
    "\n",
    "# no inningHighlights.csv, events.csv, pitches.csv\n",
    "with open(os.path.join('data', year, 'no_events.txt'), 'a') as f:\n",
    "    for gameid in no_events:\n",
    "        f.write(gameid+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game</th>\n",
       "      <th>away</th>\n",
       "      <th>away-record</th>\n",
       "      <th>awayaway-record</th>\n",
       "      <th>home</th>\n",
       "      <th>home-record</th>\n",
       "      <th>homehome-record</th>\n",
       "      <th>away-score</th>\n",
       "      <th>home-score</th>\n",
       "      <th>postseason info</th>\n",
       "      <th>...</th>\n",
       "      <th>LOSS - Pitcher - Id</th>\n",
       "      <th>LOSS - Pitcher - Name</th>\n",
       "      <th>LOSS - Pitcher - AbbrName</th>\n",
       "      <th>LOSS - Pitcher - Record</th>\n",
       "      <th>SAVE - Pitcher - Stats</th>\n",
       "      <th>SAVE - Pitcher - Id</th>\n",
       "      <th>SAVE - Pitcher - Name</th>\n",
       "      <th>SAVE - Pitcher - AbbrName</th>\n",
       "      <th>SAVE - Pitcher - Record</th>\n",
       "      <th>Extra Innings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>401228837</td>\n",
       "      <td>CLE</td>\n",
       "      <td>57-60</td>\n",
       "      <td>28-33 Away</td>\n",
       "      <td>MIN</td>\n",
       "      <td>53-66</td>\n",
       "      <td>29-32 Home</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>33267.0</td>\n",
       "      <td>Nick Wittgren</td>\n",
       "      <td>N. Wittgren</td>\n",
       "      <td>(2-6)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>401228838</td>\n",
       "      <td>HOU</td>\n",
       "      <td>70-48</td>\n",
       "      <td>33-25 Away</td>\n",
       "      <td>KC</td>\n",
       "      <td>50-67</td>\n",
       "      <td>30-30 Home</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>32888.0</td>\n",
       "      <td>Yimi Garcia</td>\n",
       "      <td>Y. Garcia</td>\n",
       "      <td>(3-8)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>401289753</td>\n",
       "      <td>SD</td>\n",
       "      <td>67-54</td>\n",
       "      <td>27-30 Away</td>\n",
       "      <td>COL</td>\n",
       "      <td>53-66</td>\n",
       "      <td>39-21 Home</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>30376.0</td>\n",
       "      <td>Daniel Hudson</td>\n",
       "      <td>D. Hudson</td>\n",
       "      <td>(4-2)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>401228839</td>\n",
       "      <td>NYM</td>\n",
       "      <td>59-59</td>\n",
       "      <td>23-36 Away</td>\n",
       "      <td>SF</td>\n",
       "      <td>77-42</td>\n",
       "      <td>41-18 Home</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>33820.0</td>\n",
       "      <td>Miguel Castro</td>\n",
       "      <td>M. Castro</td>\n",
       "      <td>(3-4)</td>\n",
       "      <td>1.0 IP, 0 ER, 1 K, 0 BB</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>Jake McGee</td>\n",
       "      <td>J. McGee</td>\n",
       "      <td>(26)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>401228841</td>\n",
       "      <td>PIT</td>\n",
       "      <td>42-77</td>\n",
       "      <td>18-42 Away</td>\n",
       "      <td>LAD</td>\n",
       "      <td>73-46</td>\n",
       "      <td>37-20 Home</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>33014.0</td>\n",
       "      <td>Chasen Shreve</td>\n",
       "      <td>C. Shreve</td>\n",
       "      <td>(1-1)</td>\n",
       "      <td>1.0 IP, 0 ER, 1 K, 0 BB</td>\n",
       "      <td>29630.0</td>\n",
       "      <td>Kenley Jansen</td>\n",
       "      <td>K. Jansen</td>\n",
       "      <td>(24)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Game away away-record awayaway-record home home-record  \\\n",
       "1785  401228837  CLE       57-60      28-33 Away  MIN       53-66   \n",
       "1786  401228838  HOU       70-48      33-25 Away   KC       50-67   \n",
       "1787  401289753   SD       67-54      27-30 Away  COL       53-66   \n",
       "1788  401228839  NYM       59-59      23-36 Away   SF       77-42   \n",
       "1789  401228841  PIT       42-77      18-42 Away  LAD       73-46   \n",
       "\n",
       "     homehome-record  away-score  home-score postseason info  ...  \\\n",
       "1785      29-32 Home           4           5             NaN  ...   \n",
       "1786      30-30 Home           6           7             NaN  ...   \n",
       "1787      39-21 Home           5           6             NaN  ...   \n",
       "1788      41-18 Home           5           7             NaN  ...   \n",
       "1789      37-20 Home           1           2             NaN  ...   \n",
       "\n",
       "      LOSS - Pitcher - Id  LOSS - Pitcher - Name  LOSS - Pitcher - AbbrName  \\\n",
       "1785              33267.0          Nick Wittgren                N. Wittgren   \n",
       "1786              32888.0            Yimi Garcia                  Y. Garcia   \n",
       "1787              30376.0          Daniel Hudson                  D. Hudson   \n",
       "1788              33820.0          Miguel Castro                  M. Castro   \n",
       "1789              33014.0          Chasen Shreve                  C. Shreve   \n",
       "\n",
       "      LOSS - Pitcher - Record   SAVE - Pitcher - Stats  SAVE - Pitcher - Id  \\\n",
       "1785                    (2-6)                      NaN                  NaN   \n",
       "1786                    (3-8)                      NaN                  NaN   \n",
       "1787                    (4-2)                      NaN                  NaN   \n",
       "1788                    (3-4)  1.0 IP, 0 ER, 1 K, 0 BB              28959.0   \n",
       "1789                    (1-1)  1.0 IP, 0 ER, 1 K, 0 BB              29630.0   \n",
       "\n",
       "      SAVE - Pitcher - Name  SAVE - Pitcher - AbbrName  \\\n",
       "1785                    NaN                        NaN   \n",
       "1786                    NaN                        NaN   \n",
       "1787                    NaN                        NaN   \n",
       "1788             Jake McGee                   J. McGee   \n",
       "1789          Kenley Jansen                  K. Jansen   \n",
       "\n",
       "     SAVE - Pitcher - Record Extra Innings  \n",
       "1785                     NaN          True  \n",
       "1786                     NaN           NaN  \n",
       "1787                     NaN           NaN  \n",
       "1788                    (26)           NaN  \n",
       "1789                    (24)           NaN  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "games_df = pd.read_csv(os.path.join('data', year, 'games.csv'))\n",
    "games_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num</th>\n",
       "      <th>Pitch</th>\n",
       "      <th>Type</th>\n",
       "      <th>MPH</th>\n",
       "      <th>play-hitzone</th>\n",
       "      <th>play-bases</th>\n",
       "      <th>play-field</th>\n",
       "      <th>Pitcher</th>\n",
       "      <th>Pitching Team</th>\n",
       "      <th>Batting Team</th>\n",
       "      <th>Inning</th>\n",
       "      <th>Event Id</th>\n",
       "      <th>Game</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>522434</th>\n",
       "      <td>4</td>\n",
       "      <td>Double</td>\n",
       "      <td>Sinker</td>\n",
       "      <td>95</td>\n",
       "      <td>top: 9.5px; right: 17.22px;</td>\n",
       "      <td>2.0</td>\n",
       "      <td>top: 11.38px; right: 28.12px;</td>\n",
       "      <td>Jansen</td>\n",
       "      <td>LAD</td>\n",
       "      <td>PIT</td>\n",
       "      <td>Top 9th</td>\n",
       "      <td>97</td>\n",
       "      <td>401228841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522435</th>\n",
       "      <td>1</td>\n",
       "      <td>Strike Looking</td>\n",
       "      <td>Cutter</td>\n",
       "      <td>93</td>\n",
       "      <td>top: 15.14px; right: 19.67px;</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jansen</td>\n",
       "      <td>LAD</td>\n",
       "      <td>PIT</td>\n",
       "      <td>Top 9th</td>\n",
       "      <td>98</td>\n",
       "      <td>401228841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522436</th>\n",
       "      <td>2</td>\n",
       "      <td>Ground Out</td>\n",
       "      <td>Cutter</td>\n",
       "      <td>93</td>\n",
       "      <td>top: 12.75px; right: 27.2px;</td>\n",
       "      <td>3.0</td>\n",
       "      <td>top: 16.95px; right: 16.52px;</td>\n",
       "      <td>Jansen</td>\n",
       "      <td>LAD</td>\n",
       "      <td>PIT</td>\n",
       "      <td>Top 9th</td>\n",
       "      <td>98</td>\n",
       "      <td>401228841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522437</th>\n",
       "      <td>1</td>\n",
       "      <td>Strike Looking</td>\n",
       "      <td>Cutter</td>\n",
       "      <td>93</td>\n",
       "      <td>top: 14.27px; right: 19.06px;</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jansen</td>\n",
       "      <td>LAD</td>\n",
       "      <td>PIT</td>\n",
       "      <td>Top 9th</td>\n",
       "      <td>99</td>\n",
       "      <td>401228841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522438</th>\n",
       "      <td>2</td>\n",
       "      <td>Ground Out</td>\n",
       "      <td>Sinker</td>\n",
       "      <td>95</td>\n",
       "      <td>top: 14.71px; right: 20.89px;</td>\n",
       "      <td>3.0</td>\n",
       "      <td>top: 15.33px; right: 16.4px;</td>\n",
       "      <td>Jansen</td>\n",
       "      <td>LAD</td>\n",
       "      <td>PIT</td>\n",
       "      <td>Top 9th</td>\n",
       "      <td>99</td>\n",
       "      <td>401228841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Num           Pitch    Type MPH                   play-hitzone  \\\n",
       "522434    4          Double  Sinker  95    top: 9.5px; right: 17.22px;   \n",
       "522435    1  Strike Looking  Cutter  93  top: 15.14px; right: 19.67px;   \n",
       "522436    2      Ground Out  Cutter  93   top: 12.75px; right: 27.2px;   \n",
       "522437    1  Strike Looking  Cutter  93  top: 14.27px; right: 19.06px;   \n",
       "522438    2      Ground Out  Sinker  95  top: 14.71px; right: 20.89px;   \n",
       "\n",
       "        play-bases                     play-field Pitcher Pitching Team  \\\n",
       "522434         2.0  top: 11.38px; right: 28.12px;  Jansen           LAD   \n",
       "522435         2.0                            NaN  Jansen           LAD   \n",
       "522436         3.0  top: 16.95px; right: 16.52px;  Jansen           LAD   \n",
       "522437         3.0                            NaN  Jansen           LAD   \n",
       "522438         3.0   top: 15.33px; right: 16.4px;  Jansen           LAD   \n",
       "\n",
       "       Batting Team   Inning  Event Id       Game  \n",
       "522434          PIT  Top 9th        97  401228841  \n",
       "522435          PIT  Top 9th        98  401228841  \n",
       "522436          PIT  Top 9th        98  401228841  \n",
       "522437          PIT  Top 9th        99  401228841  \n",
       "522438          PIT  Top 9th        99  401228841  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitches_df = pd.read_csv(os.path.join('data', year, 'pitches.csv'))\n",
    "pitches_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (9,10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\Joseph\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (2,3,4,5,6,7,8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\Joseph\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\Joseph\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Combines the seasons into a 'total' folder\n",
    "first_year = 2016\n",
    "last_year = 2021\n",
    "\n",
    "# make sure the path exists\n",
    "path = os.path.join('data', 'total')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# go through each file\n",
    "for file in os.listdir(os.path.join('data', '2020')):\n",
    "    if '.csv' not in file:\n",
    "        continue\n",
    "    # go through each year\n",
    "    for year in range(first_year, last_year+1): \n",
    "        new_df = pd.read_csv(os.path.join('data', str(year), file))\n",
    "        \n",
    "        if year == first_year:\n",
    "            rolling_df = new_df\n",
    "        else:\n",
    "            rolling_df = rolling_df.append(new_df, ignore_index=True)\n",
    "    \n",
    "    # write it\n",
    "    rolling_df.to_csv(os.path.join(path, file), index=False)\n",
    "rolling_df = None\n",
    "new_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ball                                             187540\n",
       "Foul Ball                                         91817\n",
       "Strike Looking                                    86676\n",
       "Strike Swinging                                   64661\n",
       "Ground Out                                        25145\n",
       "Single                                            18098\n",
       "Fly Out                                           15338\n",
       "Line Out                                           7459\n",
       "Double                                             5440\n",
       "Home Run                                           4328\n",
       "Pop Out                                            3881\n",
       "Batters Fielders Choice (runner Out)               2976\n",
       "Foul Out                                           2417\n",
       "Hit By Pitch                                       1560\n",
       "Bunted Foul                                        1201\n",
       "Batter Reached On Error (batter To First)           946\n",
       "Sacrifice Fly                                       819\n",
       "Sacrifice                                           568\n",
       "Triple                                              474\n",
       "Batters Fielders Choice (all Runners Safe)          314\n",
       "Ground Rule Double                                  287\n",
       "Bunt Single                                         237\n",
       "Bunt Ground Out                                     187\n",
       "Bunt Pop Out                                         54\n",
       "Wild Pitch; Runner Reached                            9\n",
       "Catchers Interference (batter To First/error)         3\n",
       "Strikeout Batter Safe, Passed Ball                    2\n",
       "Bunt Double                                           1\n",
       "Intentional Ball                                      1\n",
       "Name: Pitch, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strike_calls = ['Strike Looking', 'Strike Swinging', 'Foul Ball', \n",
    "                'Ground Out', 'Fly Out', 'Line Out', 'Pop Out', \n",
    "                'Foul Out', 'Bunted Foul', 'Sacrifice Fly', 'Sacrifice', \n",
    "                'Batters Fielders Choice (runner Out)', 'Bunt Ground Out', \n",
    "                'Bunt Pop Out', 'Strikeout Batter Safe, Passed Ball']\n",
    "hit_calls = ['Single', 'Double', 'Triple', 'Home Run', 'Ground Rule Double', \n",
    "             'Bunt Single', 'Bunt Double', 'Inside The Park Home Run']\n",
    "ball_calls = ['Ball', 'Hit By Pitch', 'Wild Pitch; Runner Reached', 'Intentional Ball']\n",
    "error_calls = ['Batter Reached On Error (batter To First)', 'Catchers Interference (batter To First/error)']\n",
    "other_calls = ['Batters Fielders Choice (all Runners Safe)', 'Batters Interference (batter Out)', \n",
    "               'Official Ruling Pending'] # ?\n",
    "\n",
    "pitches_df['Pitch'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
